{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from concrete_dropout import ConcreteDropout\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17248,), (17248, 11))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.isnull().any(axis=1).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"truth_X_pd.csv\")\n",
    "Y = pd.read_csv(\"truth_Y_pd.csv\")\n",
    "nan_rows = Y.isnull().any(axis=1).values\n",
    "X = X.loc[~nan_rows, :]#.values\n",
    "Y = Y.loc[~nan_rows, :]#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteDropout(nn.Module):\n",
    "    def __init__(self, weight_regularizer=1e-6,\n",
    "                 dropout_regularizer=1e-5, init_min=0.1, init_max=0.1):\n",
    "        super(ConcreteDropout, self).__init__()\n",
    "\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        \n",
    "        init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "        \n",
    "        self.p_logit = nn.Parameter(torch.empty(1).uniform_(init_min, init_max))\n",
    "        \n",
    "    def forward(self, x, layer):\n",
    "        p = torch.sigmoid(self.p_logit)\n",
    "        \n",
    "        out = layer(self._concrete_dropout(x, p))\n",
    "        \n",
    "        sum_of_square = 0\n",
    "        for param in layer.parameters():\n",
    "            sum_of_square += torch.sum(torch.pow(param, 2))\n",
    "        \n",
    "        weights_regularizer = self.weight_regularizer * sum_of_square / (1 - p)\n",
    "        \n",
    "        dropout_regularizer = p * torch.log(p)\n",
    "        dropout_regularizer += (1. - p) * torch.log(1. - p)\n",
    "        \n",
    "        input_dimensionality = x[0].numel() # Number of elements of first item in batch\n",
    "        dropout_regularizer *= self.dropout_regularizer * input_dimensionality\n",
    "        \n",
    "        regularization = weights_regularizer + dropout_regularizer\n",
    "        return out, regularization\n",
    "        \n",
    "    def _concrete_dropout(self, x, p):\n",
    "        eps = 1e-7\n",
    "        temp = 0.1\n",
    "\n",
    "        unif_noise = torch.rand_like(x)\n",
    "\n",
    "        drop_prob = (torch.log(p + eps)\n",
    "                    - torch.log(1 - p + eps)\n",
    "                    + torch.log(unif_noise + eps)\n",
    "                    - torch.log(1 - unif_noise + eps))\n",
    "        \n",
    "        drop_prob = torch.sigmoid(drop_prob / temp)\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - p\n",
    "        \n",
    "        x  = torch.mul(x, random_tensor)\n",
    "        x /= retain_prob\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, 15519)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = int(X.shape[0]*0.9) # Number of data points\n",
    "nb_epoch = 20\n",
    "nb_val_size = X.shape[0] - N # Validation size\n",
    "nb_features = 20 # Hidden layer size\n",
    "Q = 9 # Data dimensionality\n",
    "D = 11 # One mean, one log_var\n",
    "K_test = 20 # Number of MC samples\n",
    "nb_reps = 3 # Number of times to repeat experiment\n",
    "batch_size = 10\n",
    "l = 1e-4 # Lengthscale\n",
    "nb_val_size, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, nb_features, weight_regularizer, dropout_regularizer):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(Q, nb_features)\n",
    "        self.linear2 = nn.Linear(nb_features, nb_features)\n",
    "        self.linear3 = nn.Linear(nb_features, nb_features)\n",
    "\n",
    "        self.linear4_mu = nn.Linear(nb_features, D)\n",
    "        self.linear4_logvar = nn.Linear(nb_features, D)\n",
    "\n",
    "        self.conc_drop1 = ConcreteDropout(weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop2 = ConcreteDropout(weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop3 = ConcreteDropout(weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop_mu = ConcreteDropout(weight_regularizer=weight_regularizer,\n",
    "                                             dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop_logvar = ConcreteDropout(weight_regularizer=weight_regularizer,\n",
    "                                                 dropout_regularizer=dropout_regularizer)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        regularization = torch.empty(5, device=x.device)\n",
    "        \n",
    "        x1, regularization[0] = self.conc_drop1(x, nn.Sequential(self.linear1, self.relu))\n",
    "        x2, regularization[1] = self.conc_drop2(x1, nn.Sequential(self.linear2, self.relu))\n",
    "        x3, regularization[2] = self.conc_drop3(x2, nn.Sequential(self.linear3, self.relu))\n",
    "\n",
    "        mean, regularization[3] = self.conc_drop_mu(x3, self.linear4_mu)\n",
    "        log_var, regularization[4] = self.conc_drop_logvar(x3, self.linear4_logvar)\n",
    "\n",
    "        return mean, log_var, regularization.sum()\n",
    "\n",
    "def heteroscedastic_loss(true, mean, log_var):\n",
    "    precision = torch.exp(-log_var)\n",
    "    return torch.mean(torch.sum(precision * (true - mean)**2 + log_var, 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(nb_epoch, X, Y):\n",
    "    N = X.shape[0]\n",
    "    wr = l**2. / N\n",
    "    dr = 2. / N\n",
    "    model = Model(nb_features, wr, dr)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for i in range(nb_epoch):\n",
    "        old_batch = 0\n",
    "        for batch in range(int(np.ceil(X.shape[0]/batch_size))):\n",
    "            batch = (batch + 1)\n",
    "            _x = X[old_batch: batch_size*batch]\n",
    "            _y = Y[old_batch: batch_size*batch]\n",
    "            \n",
    "            x = Variable(torch.FloatTensor(_x)).to(device)\n",
    "            y = Variable(torch.FloatTensor(_y)).to(device)\n",
    "            \n",
    "            mean, log_var, regularization = model(x)\n",
    "                        \n",
    "            loss = heteroscedastic_loss(y, mean, log_var) + regularization\n",
    "             \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(a):\n",
    "    a_max = a.max(axis=0)\n",
    "    return np.log(np.sum(np.exp(a - a_max), axis=0)) + a_max\n",
    "\n",
    "def test(Y_true, K_test, means, logvar):\n",
    "    \"\"\"\n",
    "    Estimate predictive log likelihood:\n",
    "    log p(y|x, D) = log int p(y|x, w) p(w|D) dw\n",
    "                 ~= log int p(y|x, w) q(w) dw\n",
    "                 ~= log 1/K sum p(y|x, w_k) with w_k sim q(w)\n",
    "                  = LogSumExp log p(y|x, w_k) - log K\n",
    "    :Y_true: a 2D array of size N x dim\n",
    "    :MC_samples: a 3D array of size samples K x N x 2*D\n",
    "    \"\"\"\n",
    "    k = K_test\n",
    "    N = Y_true.shape[0]\n",
    "    mean = means \n",
    "    logvar = logvar\n",
    "    test_ll = -0.5 * np.exp(-logvar) * (mean - Y_val.squeeze())**2. - 0.5 * logvar - 0.5 * np.log(2 * np.pi) #Y_true[None]\n",
    "    test_ll = np.sum(np.sum(test_ll, -1), -1)\n",
    "    test_ll = logsumexp(test_ll) - np.log(k)\n",
    "    pppp = test_ll / N  # per point predictive probability\n",
    "    rmse = np.mean((np.mean(mean, 0) - Y_val.squeeze())**2.)**0.5\n",
    "    return pppp, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X_train, Y_train, X_val, Y_val, means, param):\n",
    "    # means ~ [K_test, data_size, num_params]\n",
    "    # Number of repeated sampling\n",
    "    K_test = means.shape[0]\n",
    "    indx = np.argsort(X_val[:, param]) # plt.plot requires x-ordered points\n",
    "    _, (ax1, ax2, ax3, ax4) = pylab.subplots(1, 4,figsize=(12, 1.5), sharex=True, sharey=True)\n",
    "    # 1. Show just the train set\n",
    "    ax1.scatter(X_train[:, param], Y_train[:, param], c='y')\n",
    "    ax1.set_title('Train set')\n",
    "    # 1 overlaid with 2. Show predicted means (averaged across samples) for validation set\n",
    "    ax2.plot(X_val[indx, param], np.mean(means, 0)[indx, param], color='skyblue', lw=3)\n",
    "    ax2.scatter(X_train[:, param], Y_train[:, 0], c='y')\n",
    "    ax2.set_title('+Predictive mean')\n",
    "    # 2 overlaid with 3. All predicted means across samples (not averaged) for validation set\n",
    "    for i in range(K_test):\n",
    "        ax3.scatter(X_val[:, param], means[i, :, param], c='b', alpha=0.2, lw=0)\n",
    "    ax3.plot(X_val[indx, param], np.mean(means, 0)[indx, param], color='skyblue', lw=3)\n",
    "    ax3.set_title('+MC samples on validation X')\n",
    "    # Just the validation set\n",
    "    ax4.scatter(X_val[:, param], Y_val[:, param], c='r', alpha=0.2, lw=0)\n",
    "    ax4.set_title('Validation set')\n",
    "    \n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = gen_data(N + nb_val_size)\n",
    "X_train, Y_train = X[:N], Y[:N]\n",
    "X_val, Y_val = X[N:], Y[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_model(nb_epoch, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (linear1): Linear(in_features=9, out_features=1024, bias=True)\n",
       "  (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (linear3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (linear4_mu): Linear(in_features=1024, out_features=11, bias=True)\n",
       "  (linear4_logvar): Linear(in_features=1024, out_features=11, bias=True)\n",
       "  (conc_drop1): ConcreteDropout()\n",
       "  (conc_drop2): ConcreteDropout()\n",
       "  (conc_drop3): ConcreteDropout()\n",
       "  (conc_drop_mu): ConcreteDropout()\n",
       "  (conc_drop_logvar): ConcreteDropout()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_samples = [model(Variable(torch.FloatTensor(X_val)).to(device)) for _ in range(K_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2566, 8]), 20, 2566)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MC_samples[0][0].shape, len(MC_samples), X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2566, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([tup[0] for tup in MC_samples]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = torch.stack([tup[0] for tup in MC_samples]).view(K_test, X_val.shape[0], D).cpu().data.numpy()\n",
    "logvar = torch.stack([tup[1] for tup in MC_samples]).view(K_test, X_val.shape[0], D).cpu().data.numpy()\n",
    "pppp, rmse = test(Y_val, K_test, means, logvar)\n",
    "param = 4\n",
    "epistemic_uncertainty = np.var(means, 0).mean(0)[param]\n",
    "#logvar = np.mean(logvar, 0)[param]\n",
    "aleatoric_uncertainty = np.exp(logvar).mean(0)\n",
    "ps = np.array([torch.sigmoid(module.p_logit).cpu().data.numpy()[0] for module in model.modules() if hasattr(module, 'p_logit')])\n",
    "#plot(X_train, Y_train, X_val, Y_val, means, param=param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"means_run2\", means.reshape(20, -1))\n",
    "np.save(\"logvar_run2\", logvar.reshape(20, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2566, 8)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logvar, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
